votingData <- ddply(electURLs, "ID", extract)
## ------------------------------------------------------------------------
##
## Clean the Data
#
#  1. Rename the votingData, first removing the 4 column which is blank
#  2. Merge the Electoral Names data with the voting data
#  3. Write out the cleansed data
votingData <- votingData[c(-4)]
names(votingData) <- c("ID", "Party", "PartyVotes", "MPName", "MPParty", "MPVotes")
cleanData <- merge(electNames, votingData)
write.csv(cleanData, file="clean_electorate_data.csv")
rm(votingData)
rm(cleanData)
rm(electNames)
rm(electURLs)
library(knitr)
library(ggplot2)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_exploration")
data2014 <- read.csv("clean_electorate_data.csv")
names(data2014)
## Data Analysis and Statistical Inference
#
# Course Project, Sept 2014
# Duke University via Coursera
# Nick Burns
##
##install.packages("XML")
##install.packages("RCurl")
library(XML)
library(plyr)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_extraction")
## ------------------------------------------------------------------------
##
# extract
# Is a function which takes a dataframe (ID, url)
# and returns a dataframe of the election data for the electorate
# This function will be passed to ddply later on
extract <- function(x) {
# 1. read in the HTML and extract the table data (td elements)
# 2. coerce the data into a matrix and transpose such that for each row we have 6 variables
#    those variables are:
#       {"ID", PartyName", "PartyVotes", "Blank", "CandidateName", "CandidateParty", "CandidateVotes"}
doc <- htmlTreeParse(x$url, useInternalNodes=T)
rawData <- c(   xpathSApply(doc, "//table/tr[contains(@class, 'hhevy')]/td", xmlValue)
, xpathSApply(doc, "//table/tr[contains(@class, 'hlite')]/td", xmlValue))
lclData <- as.data.frame(t(matrix(rawData, nrow=6, ncol=length(rawData)/6)))
rm(doc)
rm(rawData)
return (lclData)
}
## ------------------------------------------------------------------------
##
## Extract Electorate Names, Numbers and linked pages
#  This section prepares the various elements we will require
# 1. Extract full HTML document
# 2. Extract the hyperlinks themselves, but only those for the elcetorate data
#    The electorate data links all have a pattern like: 'electorate-XX.html'
#    We will filter based on a pattern like: 'electorate-' using grep
#
# NOTE: grep creates a ordinal index where TRUE. So we are filtering all links
#       based on their position in the vector which coincide with the 'electorate-' pattern
#
# 3. Create the full url address, and associate this with an electoral index
#
# 4. Use XQuery to extract the Electorate names from the hyperlinks, creating a dataframe of {ID, Electorate}
electURL <- "http://www.electionresults.govt.nz/electionresults_2014/electorateindex.html"
electDOC <- htmlTreeParse(electURL, useInternalNodes = TRUE)
rawLinks <- xpathSApply(electDOC, '//a/@href')
electLinks <- rawLinks[grep("electorate-", rawLinks)]
electURLs <- paste("http://www.electionresults.govt.nz/electionresults_2014/", electLinks, sep='')
electURLs <- data.frame(ID=c(1:length(electURLs)), url=electURLs)
electNames <- xpathSApply(electDOC, "//a[contains(@href,'electorate-')]", xmlValue)
electNames <- data.frame(ID = c(1:length(electNames)), Electorate=electNames)
# clean up
rm(electURL)
rm(electDOC)
rm(electLinks)
rm(rawLinks)
## ------------------------------------------------------------------------
##
## Gather the data for each electorate
#
#  Using the URLs gathered above, we pass these to extract()
#  Extract() reads in the data of interest, coercing it into a data frame of variables
#  ddply allows us to repeat this for all electorates (psuedo-setbased)
votingData <- ddply(electURLs, "ID", extract)
## ------------------------------------------------------------------------
##
## Clean the Data
#
#  1. Rename the votingData, first removing the 4 column which is blank
#  2. Merge the Electoral Names data with the voting data
#  3. Write out the cleansed data
votingData <- votingData[c(-4)]
names(votingData) <- c("ID", "Party", "PartyVotes", "MPName", "MPParty", "MPVotes")
cleanData <- merge(electNames, votingData)
write.csv(cleanData[, c(-1)], file="clean_electorate_data.csv")
rm(votingData)
rm(cleanData)
rm(electNames)
rm(electURLs)
library(knitr)
library(ggplot2)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_exploration")
data2014 <- read.csv("clean_electorate_data.csv")
head(data2014)
## Data Analysis and Statistical Inference
#
# Course Project, Sept 2014
# Duke University via Coursera
# Nick Burns
##
##install.packages("XML")
##install.packages("RCurl")
library(XML)
library(plyr)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_extraction")
## ------------------------------------------------------------------------
##
# extract
# Is a function which takes a dataframe (ID, url)
# and returns a dataframe of the election data for the electorate
# This function will be passed to ddply later on
extract <- function(x) {
# 1. read in the HTML and extract the table data (td elements)
# 2. coerce the data into a matrix and transpose such that for each row we have 6 variables
#    those variables are:
#       {"ID", PartyName", "PartyVotes", "Blank", "CandidateName", "CandidateParty", "CandidateVotes"}
doc <- htmlTreeParse(x$url, useInternalNodes=T)
rawData <- c(   xpathSApply(doc, "//table/tr[contains(@class, 'hhevy')]/td", xmlValue)
, xpathSApply(doc, "//table/tr[contains(@class, 'hlite')]/td", xmlValue))
lclData <- as.data.frame(t(matrix(rawData, nrow=6, ncol=length(rawData)/6)))
rm(doc)
rm(rawData)
return (lclData)
}
## ------------------------------------------------------------------------
##
## Extract Electorate Names, Numbers and linked pages
#  This section prepares the various elements we will require
# 1. Extract full HTML document
# 2. Extract the hyperlinks themselves, but only those for the elcetorate data
#    The electorate data links all have a pattern like: 'electorate-XX.html'
#    We will filter based on a pattern like: 'electorate-' using grep
#
# NOTE: grep creates a ordinal index where TRUE. So we are filtering all links
#       based on their position in the vector which coincide with the 'electorate-' pattern
#
# 3. Create the full url address, and associate this with an electoral index
#
# 4. Use XQuery to extract the Electorate names from the hyperlinks, creating a dataframe of {ID, Electorate}
electURL <- "http://www.electionresults.govt.nz/electionresults_2014/electorateindex.html"
electDOC <- htmlTreeParse(electURL, useInternalNodes = TRUE)
rawLinks <- xpathSApply(electDOC, '//a/@href')
electLinks <- rawLinks[grep("electorate-", rawLinks)]
electURLs <- paste("http://www.electionresults.govt.nz/electionresults_2014/", electLinks, sep='')
electURLs <- data.frame(ID=c(1:length(electURLs)), url=electURLs)
electNames <- xpathSApply(electDOC, "//a[contains(@href,'electorate-')]", xmlValue)
electNames <- data.frame(ID = c(1:length(electNames)), Electorate=electNames)
# clean up
rm(electURL)
rm(electDOC)
rm(electLinks)
rm(rawLinks)
## ------------------------------------------------------------------------
##
## Gather the data for each electorate
#
#  Using the URLs gathered above, we pass these to extract()
#  Extract() reads in the data of interest, coercing it into a data frame of variables
#  ddply allows us to repeat this for all electorates (psuedo-setbased)
votingData <- ddply(electURLs, "ID", extract)
## ------------------------------------------------------------------------
##
## Clean the Data
#
#  1. Rename the votingData, first removing the 4 column which is blank
#  2. Merge the Electoral Names data with the voting data
#  3. Write out the cleansed data
votingData <- votingData[c(-4)]
names(votingData) <- c("ID", "Party", "PartyVotes", "MPName", "MPParty", "MPVotes")
cleanData <- merge(electNames, votingData)
write.csv(cleanData[, "X"], file="clean_electorate_data.csv")
rm(votingData)
rm(cleanData)
rm(electNames)
rm(electURLs)
## Data Analysis and Statistical Inference
#
# Course Project, Sept 2014
# Duke University via Coursera
# Nick Burns
##
##install.packages("XML")
##install.packages("RCurl")
library(XML)
library(plyr)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_extraction")
## ------------------------------------------------------------------------
##
# extract
# Is a function which takes a dataframe (ID, url)
# and returns a dataframe of the election data for the electorate
# This function will be passed to ddply later on
extract <- function(x) {
# 1. read in the HTML and extract the table data (td elements)
# 2. coerce the data into a matrix and transpose such that for each row we have 6 variables
#    those variables are:
#       {"ID", PartyName", "PartyVotes", "Blank", "CandidateName", "CandidateParty", "CandidateVotes"}
doc <- htmlTreeParse(x$url, useInternalNodes=T)
rawData <- c(   xpathSApply(doc, "//table/tr[contains(@class, 'hhevy')]/td", xmlValue)
, xpathSApply(doc, "//table/tr[contains(@class, 'hlite')]/td", xmlValue))
lclData <- as.data.frame(t(matrix(rawData, nrow=6, ncol=length(rawData)/6)))
rm(doc)
rm(rawData)
return (lclData)
}
## ------------------------------------------------------------------------
##
## Extract Electorate Names, Numbers and linked pages
#  This section prepares the various elements we will require
# 1. Extract full HTML document
# 2. Extract the hyperlinks themselves, but only those for the elcetorate data
#    The electorate data links all have a pattern like: 'electorate-XX.html'
#    We will filter based on a pattern like: 'electorate-' using grep
#
# NOTE: grep creates a ordinal index where TRUE. So we are filtering all links
#       based on their position in the vector which coincide with the 'electorate-' pattern
#
# 3. Create the full url address, and associate this with an electoral index
#
# 4. Use XQuery to extract the Electorate names from the hyperlinks, creating a dataframe of {ID, Electorate}
electURL <- "http://www.electionresults.govt.nz/electionresults_2014/electorateindex.html"
electDOC <- htmlTreeParse(electURL, useInternalNodes = TRUE)
rawLinks <- xpathSApply(electDOC, '//a/@href')
electLinks <- rawLinks[grep("electorate-", rawLinks)]
electURLs <- paste("http://www.electionresults.govt.nz/electionresults_2014/", electLinks, sep='')
electURLs <- data.frame(ID=c(1:length(electURLs)), url=electURLs)
electNames <- xpathSApply(electDOC, "//a[contains(@href,'electorate-')]", xmlValue)
electNames <- data.frame(ID = c(1:length(electNames)), Electorate=electNames)
# clean up
rm(electURL)
rm(electDOC)
rm(electLinks)
rm(rawLinks)
## ------------------------------------------------------------------------
##
## Gather the data for each electorate
#
#  Using the URLs gathered above, we pass these to extract()
#  Extract() reads in the data of interest, coercing it into a data frame of variables
#  ddply allows us to repeat this for all electorates (psuedo-setbased)
votingData <- ddply(electURLs, "ID", extract)
## ------------------------------------------------------------------------
##
## Clean the Data
#
#  1. Rename the votingData, first removing the 4 column which is blank
#  2. Merge the Electoral Names data with the voting data
#  3. Write out the cleansed data
votingData <- votingData[c(-4)]
names(votingData) <- c("ID", "Party", "PartyVotes", "MPName", "MPParty", "MPVotes")
cleanData <- merge(electNames, votingData)
write.csv(cleanData, file="clean_electorate_data.csv")
rm(votingData)
rm(cleanData)
rm(electNames)
rm(electURLs)
data2014 <- read.csv("clean_electorate_data.csv")
library(knitr)
library(ggplot2)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_exploration")
data2014 <- read.csv("clean_electorate_data.csv")
names(data2014)
data2014 <- read.csv("clean_electorate_data.csv")[, c(-1)]
names(data2014)
summary(data2014)
head(data2014)
data2014 <- read.csv("clean_electorate_data.csv"
, colClasses = c(numeric, character, character, numeric, character, character, numeric))[, c(-1)]
data2014 <- read.csv("clean_electorate_data.csv"
, colClasses = c(numeric, numeric, character, character, numeric, character, character, numeric))[, c(-1)]
data2014 <- read.csv("clean_electorate_data.csv"
, colClasses = c("numeric", "numeric", "character", "character", "numeric", "character", "character", "numeric"))[, c(-1)]
data2014 <- read.csv("clean_electorate_data.csv"
, colClasses = c("numeric", "character", "character", "numeric", "character", "character", "numeric"))[, c(-1)]
read.csv("clean_electorate_data.csv"
, colClasses = c("numeric", "numeric", "character", "character"
, "numeric", "character", "character", "numeric"))
data2014 <- read.csv("clean_electorate_data.csv")[, c(-1)]
data2014$ID <- as.numeric(data2014$ID)
data2014$Electorate <- as.character(data2014$Electorate)
names(data2014)
data2014$Party <- as.character(data2014$Party)
data2014$PartyVotes <- as.numeric(data2014$PartyVotes)
data2014$MPParty <- as.character(data2014$MPParty)
data2014$MPName <- as.character(data2014$MPName)
data2014$MPVotes <- as.numeric(data2014$MPVotes)
summary(data2014)
head(data2014)
data <- read.csv("clean_electorate_data.csv")[, c(-1)]
head(data)
data2014$ID <- as.numeric(as.character(data$ID))
head(data2014)
head(as.numeric(as.factor(data$ID)))
head(as.numeric(as.character(as.factor(data$ID))))
x <- "4,567"
as.numeric(x)
strtoi(x)
dataX <- read.table("clean_electorate_data.csv")[, c(-1)]
dataX <- read.table("clean_electorate_data.csv")
?gsub
gsub(",.*$", "HA", "4,576")
gsub(",", "HA", "4,576")
x <- c("3,245", "5,153", "243", "4,561,523")
gsub(",", "HA", x)
gsub(",", "", x)
## Data Analysis and Statistical Inference
#
# Course Project, Sept 2014
# Duke University via Coursera
# Nick Burns
##
##install.packages("XML")
##install.packages("RCurl")
library(XML)
library(plyr)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_extraction")
## ------------------------------------------------------------------------
##
# extract
# Is a function which takes a dataframe (ID, url)
# and returns a dataframe of the election data for the electorate
# This function will be passed to ddply later on
extract <- function(x) {
# 1. read in the HTML and extract the table data (td elements)
# 2. coerce the data into a matrix and transpose such that for each row we have 6 variables
#    those variables are:
#       {"ID", PartyName", "PartyVotes", "Blank", "CandidateName", "CandidateParty", "CandidateVotes"}
doc <- htmlTreeParse(x$url, useInternalNodes=T)
rawData <- c(   xpathSApply(doc, "//table/tr[contains(@class, 'hhevy')]/td", xmlValue)
, xpathSApply(doc, "//table/tr[contains(@class, 'hlite')]/td", xmlValue))
lclData <- as.data.frame(t(matrix(rawData, nrow=6, ncol=length(rawData)/6)))
rm(doc)
rm(rawData)
return (lclData)
}
## ------------------------------------------------------------------------
##
## Extract Electorate Names, Numbers and linked pages
#  This section prepares the various elements we will require
# 1. Extract full HTML document
# 2. Extract the hyperlinks themselves, but only those for the elcetorate data
#    The electorate data links all have a pattern like: 'electorate-XX.html'
#    We will filter based on a pattern like: 'electorate-' using grep
#
# NOTE: grep creates a ordinal index where TRUE. So we are filtering all links
#       based on their position in the vector which coincide with the 'electorate-' pattern
#
# 3. Create the full url address, and associate this with an electoral index
#
# 4. Use XQuery to extract the Electorate names from the hyperlinks, creating a dataframe of {ID, Electorate}
electURL <- "http://www.electionresults.govt.nz/electionresults_2014/electorateindex.html"
electDOC <- htmlTreeParse(electURL, useInternalNodes = TRUE)
rawLinks <- xpathSApply(electDOC, '//a/@href')
electLinks <- rawLinks[grep("electorate-", rawLinks)]
electURLs <- paste("http://www.electionresults.govt.nz/electionresults_2014/", electLinks, sep='')
electURLs <- data.frame(ID=c(1:length(electURLs)), url=electURLs)
electNames <- xpathSApply(electDOC, "//a[contains(@href,'electorate-')]", xmlValue)
electNames <- data.frame(ID = c(1:length(electNames)), Electorate=electNames)
# clean up
rm(electURL)
rm(electDOC)
rm(electLinks)
rm(rawLinks)
## ------------------------------------------------------------------------
##
## Gather the data for each electorate
#
#  Using the URLs gathered above, we pass these to extract()
#  Extract() reads in the data of interest, coercing it into a data frame of variables
#  ddply allows us to repeat this for all electorates (psuedo-setbased)
votingData <- ddply(electURLs, "ID", extract)
## ------------------------------------------------------------------------
##
## Clean the Data
#
#  1. Rename the votingData, first removing the 4 column which is blank
#  2. Merge the Electoral Names data with the voting data
#  3. Write out the cleansed data
votingData <- votingData[c(-4)]
names(votingData) <- c("ID", "Party", "PartyVotes", "MPName", "MPParty", "MPVotes")
cleanData <- merge(electNames, votingData)
# Finally, I need to tweak the vote counts to remove the commas and ensure they can
# be read in as numeric data
cleanData$PartyVotes <- gsub(",", "", cleanData$PartyVotes)
cleanData$MPVotes <- gsub(",", "", cleanData$MPVotes)
write.csv(cleanData, file="clean_electorate_data.csv")
rm(votingData)
rm(cleanData)
rm(electNames)
rm(electURLs)
dataX <- read.csv("clean_electorate_data.csv")[, c(-1)]
head(dataX)
str(dataX)
data2014 <- read.csv("clean_electorate_data.csv")[, c(-1)]
summary(data2014)
str(data2014)
data2014$Electorate <- as.factor(data2014$Electorate)
summary(data2014)
str(data2014)
data2014$ID <- as.factor(data2014$ID)
summary(data2014)
str(data2014)
head(data2104)
head(data2014)
summary(data2014)
str(data2014)
head(data2014, 10)
library(knitr)
library(ggplot2)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_exploration")
hist(data2014$PartyVotes)
hist(data2014$PartyVotes, bins=50)
?hist
hist(data2014$PartyVotes, breaks=50)
hist(data2014$MPVotes, breaks=50)
visData <- data.frame(type= c("party"), votes=data2014$PartyVote)
visData <- cbind(data.frame(type=c("MP"), votes=data2014$MPVotes))
hist(visData$votesotes, breaks=50)
hist(visData$votes, breaks=50)
g <- (visData, aes(x=votes, group=type)) + geom_histogram(stat="identity")
ggplot(visData, aes(x=votes, group=type)) + geom_histogram(stat="identity")
ggplot(visData, aes(x=votes), group=type) + geom_histogram(stat="identity")
ggplot(visData, aes(x=votes), group=type) + geom_bar(stat="identity")
dat.m <- melt(visData, "sex")
ggplot(dat.m, aes(value)) +
geom_bar(binwidth = 0.5) +
facet_grid(variable ~ type)
library(plyr)
?,etl
?melt
dat.m <- melt(visData, "sex")
library(reshape)
install.packages(reshape2)
install.packages('reshape2')
install.packages("reshape2")
library(reshape2)
install.packages('reshape2')
install.packages('reshape2')
install.packages("reshape2")
library(knitr)
library(ggplot2)
library(reshape2)
setwd("C:\\DataSciToolkit\\myGitHub\\nzelections2014\\data_exploration")
data2014 <- read.csv("clean_electorate_data.csv")[, c(-1)]    # drop the "X" column that pops up
data2014$ID <- as.factor(data2014$ID)
visData <- data.frame(type= c("party"), votes=data2014$PartyVote)
visData <- cbind(data.frame(type=c("MP"), votes=data2014$MPVotes))
dat.m <- melt(visData, "type")
ggplot(dat.m, aes(value)) +
geom_bar(binwidth = 0.5) +
facet_grid(variable ~ type)
melt_df <- melt(visData)
head(melt_df) # so you can see it
ggplot(melt_df, aes(votes, value)) +
geom_bar() +
facet_wrap(~ type)
ggplot(melt_df, aes(value)) +
geom_bar() +
facet_wrap(~ type)
melt_df
summary(melt_df)
melt_df <- melt(visData, "type")
head(melt_df) # so you can see it
summary(melt_df)
g <- ggplot(visData, aes(x=votes, y=frequency), group=type) + geom_bar(stat="identity")
g
g <- ggplot(visData, aes(x=votes, y=frequency)) + geom_bar(stat="identity")
g
hist(data2014$PartyVotes)
hist(data2014$MPVotes)
hist(data2014$PartyVotes)
hist(data2014$MPVotes)
ggplot(data2014$PartyVotes, geom="hist")
ggplot(data2014$PartyVotes, geom="histogram")
ggplot(data2014 aes(x=PartyVotes), geom="histogram")
ggplot(data2014, aes(x=PartyVotes), geom="histogram")
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="identity")
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin")
ggplot(data2014, aes(x=PartyVotes), colour="blue") +  geom_histogram(stat="bin")
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue")
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue")
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=1)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=100)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=500)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=300)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=200)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=800)
ggplot(data2014, aes(x=PartyVotes)) +  geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=1000)
ggplot(data2014, aes(x=PartyVotes)) +
geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=1000) +
ggtitle("Distribution of Party Votes in 2014 NZ General Election")
ggplot(data2014, aes(x=MPVotes)) +
geom_histogram(stat="bin", colour="blue", fill="blue", binwidth=1000) +
ggtitle("Distribution of Party Votes in 2014 NZ General Election")
byElectorate <- subset(data2014, "ID")
byElectorate <- split(data2014, "ID")
sapply(byElectorate, function(x) x[x$PartyVotes=max(x$PartyVotes), ])
sapply(byElectorate, function(x) x[(x$PartyVotes==max(x$PartyVotes)), ])
head(byElectorate)
